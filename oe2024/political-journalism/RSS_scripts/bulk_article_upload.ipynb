{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, XSD\n",
    "import uuid\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening json file\n",
    "with open('./nytimes_articles.json', 'r') as file:\n",
    "    nyt_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'National': 6135, 'Editorial': 848, 'Science': 3677, 'Games': 2045, 'Summary': 1508, 'Foreign': 15370, 'Express': 4760, 'Washington': 9199, 'Culture': 9930, 'Business': 11966, 'Magazine': 1589, 'Well': 1522, 'Travel': 955, 'Metropolitan': 888, 'Arts&Leisure': 1576, 'Photo': 10, 'Politics': 5385, 'RealEstate': 2332, 'SundayBusiness': 1057, 'Sports': 8252, 'Weekend': 3875, 'Dining': 3488, 'OpEd': 10181, 'Letters': 1585, 'Styles': 4651, 'Parenting': 879, 'Metro': 5294, 'Obits': 3883, 'BookReview': 4027, 'NYTNow': 3961, 'Climate': 1228, 'Smarter Living': 195, 'Learning': 2723, 'Insider': 184, 'Podcasts': 1323, 'TStyle': 1242, 'Upshot': 543, 'Books': 530, 'NewsDesk': 244, 'Society': 425, 'Investigative': 271, 'Video': 46, 'Universal': 26, 'Gender': 153, 'SpecialSections': 697, 'NYTI': 8, 'Live': 40, 'Neediest': 59, 'AtHome': 520, 'Graphics': 8, 'Test': 3, 'Espa√±ol': 26, 'DigitalNewsDesign': 1, 'InteractiveNews': 1, 'SundayReview': 1, 'Watching': 1, 'Headway': 13, 'Chinese': 1}\n"
     ]
    }
   ],
   "source": [
    "news_desks = {}\n",
    "for article in nyt_data:\n",
    "    if article['news_desk'] in news_desks:\n",
    "      news_desks[article['news_desk']] += 1\n",
    "    else:\n",
    "      news_desks[article['news_desk']] = 1\n",
    "print(news_desks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22795\n"
     ]
    }
   ],
   "source": [
    "news_desk_filter = {'National', 'Editorial', 'Washington','Politics', 'Climate' }\n",
    "total_articles = 0\n",
    "for desk in news_desks.keys():\n",
    "    if desk in news_desk_filter:\n",
    "        total_articles += news_desks[desk]\n",
    "print(total_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_unicode_string(s):\n",
    "    try:\n",
    "        s.encode('utf-8') \n",
    "        return True\n",
    "    except (UnicodeEncodeError, AttributeError):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ned55fa96550f448995bbcf0115b7020a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bulk uploading articles\n",
    "news_desk_filter = {'National', 'Editorial', 'Washington','Politics', 'Climate' }\n",
    "ontology_path = \"./../PoliticalJournalism-individuals-real.rdf\"\n",
    "g = Graph()\n",
    "g.parse(ontology_path, format=\"xml\")\n",
    "\n",
    "POLITICAL_JOURNALISM = Namespace(\"https://tw.rpi.edu/ontology-engineering/oe2024/political-journalism/PoliticalJournalism#\")\n",
    "COMMONS_DATES_TIMES = Namespace(\"https://www.omg.org/spec/Commons/DatesAndTimes/\")\n",
    "\n",
    "g.bind(\"PoliticalJournalism\", POLITICAL_JOURNALISM)\n",
    "g.bind(\"commonsDatesTimes\", COMMONS_DATES_TIMES)\n",
    "\n",
    "\n",
    "\n",
    "author_cache = {}\n",
    "date_cache = {}\n",
    "# Add individuals to the graph\n",
    "for article in nyt_data:\n",
    "    if article[\"abstract\"] == \"\":\n",
    "        continue\n",
    "    if not is_unicode_string(article[\"abstract\"]):\n",
    "        print(article[\"abstract\"])\n",
    "        continue\n",
    "    if article['news_desk'] not in news_desk_filter:\n",
    "        continue\n",
    "    article_label = f\"article-{uuid.uuid4()}\"\n",
    "    individual_uri = URIRef(f\"{POLITICAL_JOURNALISM}{article_label}\")\n",
    "    g.add((individual_uri, RDF.type, POLITICAL_JOURNALISM.Article))\n",
    "    g.add((individual_uri, RDFS.label, Literal(article_label)))\n",
    "    g.add((individual_uri, RDFS.comment, Literal(article[\"abstract\"])))\n",
    "    \n",
    "    g.add((individual_uri, POLITICAL_JOURNALISM.hasPublisher, POLITICAL_JOURNALISM.NewYorkTimes))\n",
    "    #g.add((individual_uri, POLITICAL_JOURNALISM.web_url, URIRef(article[\"web_url\"])))\n",
    "    parsed_date = datetime.strptime(article[\"pub_date\"], \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    formatted_date = parsed_date.strftime(\"%Y-%m-%d\")\n",
    "    if formatted_date in date_cache:\n",
    "        date_uri = date_cache[formatted_date]\n",
    "    else:\n",
    "        date_label = f\"date-{formatted_date}\"\n",
    "        date_uri = URIRef(f\"{POLITICAL_JOURNALISM}{date_label}\")\n",
    "        \n",
    "        # Check if the date individual already exists in the graph\n",
    "        if (date_uri, RDF.type, COMMONS_DATES_TIMES.ExplicitDate) not in g:\n",
    "            g.add((date_uri, RDF.type, POLITICAL_JOURNALISM.Date))\n",
    "            g.add((date_uri, RDFS.label, Literal(formatted_date)))\n",
    "            g.add((date_uri, POLITICAL_JOURNALISM.year, Literal(parsed_date.year)))\n",
    "            g.add((date_uri, POLITICAL_JOURNALISM.month, Literal(parsed_date.month)))\n",
    "            g.add((date_uri, POLITICAL_JOURNALISM.day, Literal(parsed_date.day)))\n",
    "        \n",
    "        # Cache the date individual for reuse\n",
    "        date_cache[formatted_date] = date_uri\n",
    "    # Link the Date individual to the Article individual\n",
    "    g.add((individual_uri, POLITICAL_JOURNALISM.hasPublishDate, date_uri))\n",
    "\n",
    "    for author in article[\"authors\"]:\n",
    "        author_name = author.strip()\n",
    "        if author_name in author_cache:\n",
    "            author_uri = author_cache[author_name]\n",
    "        else:\n",
    "            author_label = f\"author-{uuid.uuid4()}\"\n",
    "            author_uri = URIRef(f\"{POLITICAL_JOURNALISM}{author_label}\")\n",
    "            \n",
    "            # Create Author individual\n",
    "            g.add((author_uri, RDF.type, POLITICAL_JOURNALISM.Author))\n",
    "            g.add((author_uri, RDFS.label, Literal(author_label)))\n",
    "            g.add((author_uri, RDFS.comment, Literal(author)))\n",
    "            # Cache the author individual for reuse\n",
    "            author_cache[author_name] = author_uri\n",
    "\n",
    "        # Link the Author to the Article\n",
    "        g.add((individual_uri, POLITICAL_JOURNALISM.hasAuthor, author_uri))\n",
    "\n",
    "# Save the updated ontology\n",
    "updated_ontology_path = \"../updated_test_ontology1.rdf\"\n",
    "g.serialize(destination=updated_ontology_path, format=\"xml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
